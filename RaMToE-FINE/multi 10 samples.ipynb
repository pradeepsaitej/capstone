{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76e7b86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet transformers\n",
    "!pip install --quiet sentencepiece\n",
    "!pip install --quiet datasets\n",
    "!pip install --quiet rouge_score\n",
    "! pip install --quiet  evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbb5d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 5)\n",
      "                       id                                                url  \\\n",
      "0  international-53649907  https://www.bbc.com/telugu/international-53649907   \n",
      "1          india-46550604          https://www.bbc.com/telugu/india-46550604   \n",
      "2          india-43404438          https://www.bbc.com/telugu/india-43404438   \n",
      "3  international-54671956  https://www.bbc.com/telugu/international-54671956   \n",
      "4                53723894                https://www.bbc.com/telugu/53723894   \n",
      "\n",
      "                                               title  \\\n",
      "0  పాకిస్తాన్ ఎయిర్‌లైన్స్‌లో నకిలీ లైసెన్సుల పైల...   \n",
      "1  తెలంగాణ ముఖ్యమంత్రిగా కేసీఆర్ రెండోసారి ప్రమాణ...   \n",
      "2  ‘అధికారం కొన్ని కులాల గుప్పిట్లోనే ఉండాలా? కుద...   \n",
      "3  పోలండ్‌లో కొత్త అబార్షన్ చట్టాలను వ్యతిరేకిస్త...   \n",
      "4  దిల్లీ అల్లర్లపై పరస్పర విరుద్ధ నివేదికలు... ఏ...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  పాకిస్తాన్ విమానయాన రంగంలో కొత్త సంక్షోభం మొదల...   \n",
      "1  తెలంగాణ ముఖ్యమంత్రిగా కల్వకుంట్ల చంద్రశేఖర్ రా...   \n",
      "2  గుంటూరులో జనసేన ఆవిర్భావ దినోత్సవ సభ జరిగింది....   \n",
      "3  పోలండ్‌లోని కొత్త అబార్షన్ చట్టాలకు వ్యతిరేకంగ...   \n",
      "4  ఈ ఏడాది ఫిబ్రవరిలో జరిగిన ఢిల్లీ హింసలో ఏ వర్గ...   \n",
      "\n",
      "                                                text  \n",
      "0  అయితే, ఆ జాబితా తప్పులతడకని పైలట్లు అంటున్నారు...  \n",
      "1  ఆ తరువాత ఆయన రెండోసారి ముఖ్యమంత్రిగా బాధ్యతలు ...  \n",
      "2  ప్రత్యేక హోదా సాధన, టీడీపీపై విమర్శలే ప్రధానాస...  \n",
      "3  గత ఏడాది చట్టబద్ధంగా జరిగిన అబార్షన్లలో 98% కడ...  \n",
      "4  అయితే, ఈ అంశంపై సొంతంగా విచారణ జరిపిన ఈ కమిటీల...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"telugu\", \"urdu\", \"marathi\", \"hindi\", \"tamil\", \"bengali\", \"english\"]\n",
    "\n",
    "dfs = []\n",
    "for lang in languages:\n",
    "    dataset = load_dataset(\"csebuetnlp/xlsum\", lang, split=\"train[:10]\")\n",
    "    df = dataset.to_pandas()\n",
    "    dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfaa578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07bd641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\SRMAPCSELAB2022-147\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3890e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data=df,\n",
    "        tokenizer=tokenizer,\n",
    "        text_max_token_len = 200,\n",
    "        summary_max_token_len = 12\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        text = data_row['text']\n",
    "\n",
    "        text_encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        summary_encoding = tokenizer(\n",
    "            data_row['summary'],\n",
    "            max_length=self.summary_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = summary_encoding['input_ids']\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=text_encoding['input_ids'].flatten(),\n",
    "            attention_mask=text_encoding['attention_mask'].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            decoder_attention_mask=summary_encoding['attention_mask'].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c780c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SummaryDataset(data=df_train)\n",
    "test_dataset = SummaryDataset(data=df_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=10)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3957f4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7fe1ced0634815b9334ca62c7af06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 -- loss: 3.22409725189209\n",
      "epoch: 2 -- loss: 1.8690118789672852\n",
      "epoch: 3 -- loss: 1.137370228767395\n",
      "epoch: 4 -- loss: 0.5539778470993042\n",
      "epoch: 5 -- loss: 0.2561177909374237\n",
      "epoch: 6 -- loss: 0.09371406584978104\n",
      "epoch: 7 -- loss: 0.055544693022966385\n",
      "epoch: 8 -- loss: 0.03183036670088768\n",
      "epoch: 9 -- loss: 0.03371523320674896\n",
      "epoch: 10 -- loss: 0.012720723636448383\n",
      "epoch: 11 -- loss: 0.009863502345979214\n",
      "epoch: 12 -- loss: 0.019593294709920883\n",
      "epoch: 13 -- loss: 0.004887669812887907\n",
      "epoch: 14 -- loss: 0.005240254104137421\n",
      "epoch: 15 -- loss: 0.003453712910413742\n",
      "epoch: 16 -- loss: 0.006169792730361223\n",
      "epoch: 17 -- loss: 0.0032350411638617516\n",
      "epoch: 18 -- loss: 0.020194603130221367\n",
      "epoch: 19 -- loss: 0.002554779639467597\n",
      "epoch: 20 -- loss: 0.0015026010805740952\n",
      "epoch: 21 -- loss: 0.0017813724698498845\n",
      "epoch: 22 -- loss: 0.004608242306858301\n",
      "epoch: 23 -- loss: 0.005420275032520294\n",
      "epoch: 24 -- loss: 0.002345263259485364\n",
      "epoch: 25 -- loss: 0.005370358005166054\n",
      "epoch: 26 -- loss: 0.009397742338478565\n",
      "epoch: 27 -- loss: 0.0060935295186936855\n",
      "epoch: 28 -- loss: 0.0009778911480680108\n",
      "epoch: 29 -- loss: 0.0009782101260498166\n",
      "epoch: 30 -- loss: 0.0009050877415575087\n",
      "epoch: 31 -- loss: 0.0010965163819491863\n",
      "epoch: 32 -- loss: 0.005272946786135435\n",
      "epoch: 33 -- loss: 0.0009065424674190581\n",
      "epoch: 34 -- loss: 0.0003947982331737876\n",
      "epoch: 35 -- loss: 0.00073968869401142\n",
      "epoch: 36 -- loss: 0.0007094363099895418\n",
      "epoch: 37 -- loss: 0.0006153220892883837\n",
      "epoch: 38 -- loss: 0.0007266259635798633\n",
      "epoch: 39 -- loss: 0.004354333970695734\n",
      "epoch: 40 -- loss: 0.0006790132611058652\n",
      "epoch: 41 -- loss: 0.0013955527683719993\n",
      "epoch: 42 -- loss: 0.0007983972900547087\n",
      "epoch: 43 -- loss: 0.0011147321201860905\n",
      "epoch: 44 -- loss: 0.0004963966202922165\n",
      "epoch: 45 -- loss: 0.0008086017915047705\n",
      "epoch: 46 -- loss: 0.0008139074197970331\n",
      "epoch: 47 -- loss: 0.00044054482714273036\n",
      "epoch: 48 -- loss: 0.0082431985065341\n",
      "epoch: 49 -- loss: 0.000547492818441242\n",
      "epoch: 50 -- loss: 0.0004973129834979773\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "#         logits = outputs.logits\n",
    "#         predictions = torch.argmax(logits, dim=-1)\n",
    "#         print(predictions)\n",
    "#         print(batch[\"labels\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update()\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, f'./t5-multi.pth')\n",
    "\n",
    "    print(f'epoch: {epoch + 1} -- loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2737ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: {'rouge1': 0.15, 'rouge2': 0.0, 'rougeL': 0.15, 'rougeLsum': 0.15}\n",
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.1875, 0.0, 0.0, 0.0], 'brevity_penalty': 0.9394130628134758, 'length_ratio': 0.9411764705882353, 'translation_length': 16, 'reference_length': 17}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    all_predictions.extend(decoded_predictions)\n",
    "    all_references.extend(decoded_references)\n",
    "\n",
    "rouge_score = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "bleu_score = bleu.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "print(\"ROUGE Score:\", rouge_score)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
